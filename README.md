# Web Archiving using Conifer

Web archiving is crucial for preserving digital information, ensuring access for future research, and documenting evolving web content. Most of us have experienced the frustration of trying to access online content that no longer exists or has changed significantly from what we remember. 

There are many different tools and methods for web archiving, each producing different formats of archived web content. The three main types of web archiving are: 
Remote harvesting: Uses web crawlers to browse and capture webpages.
Database harvesting: Involves exporting data from a database into a standard schema such as XML.
Transactional harvesting: Captures web pages based on user interactions such as viewing a web page. 

For more information on these other types of web archiving and how to use web crawlers, see our Web Archiving and Data Rescue Lib Guide as well as the Virginia Tech University Libraries Web Archiving Lib Guide. 

Today we’ll be focusing on the third method: transactional harvesting using an open-source tool called Conifer. Conifer has a high success rate capturing complex websites with elements like embedded videos and 3D graphics. Its user-friendly interface makes it approachable for those new to web archiving. Another advantage is that it can capture content visible only to logged-in users, which may differ from public view. 

*Note on privacy: If you need to capture a site that requires login, consider creating a throwaway account just for web archiving. Your login credentials may be captured as data within your collection. 

Conifer provides 5GB of free storage, and access to publicly shared collections is free and unlimited. 

We’ll use the U.S. Environmental Protection Agency (EPA) website for this tutorial, but feel free to substitute any site of interest. 

